#!/usr/bin/env python
import ROOT
import os

import numpy as np
from datetime import datetime
from importlib import import_module

import click
import click_log
import yaml
import logging

from cmsl1t import logger
from cmsl1t.config import ConfigParser
from cmsl1t.filters import create_event_mask
from cmsl1t.io.eventreader import EventReader
from cmsl1t.utils.module import load_L1TNTupleLibrary
from cmsl1t.utils.timers import timerfunc_log_to

logging.getLogger("rootpy.tree.chain").setLevel(logging.WARNING)
logging.getLogger("matplotlib").setLevel(logging.WARNING)
#click_log.basic_config(logger)

TODAY = datetime.now().timetuple()
ROOT.PyConfig.IgnoreCommandLineOptions = True
ROOT.gROOT.SetBatch(1)
ROOT.TH1.SetDefaultSumw2(True)
separator = '=' * 20
section = [separator, '{0}', separator]
section = '    '.join(section)


@timerfunc_log_to(logger.info)
def process_tuples(config, nevents, analyzers, producers, filters=None):
    # Open the data files
    logger.info(section.format("Loading data"))

    input_files = config.get('input', 'files')
    logger.info("Input files:")

    if len(input_files) > 10:
        file_msg = ['    ' + '\n'.join(input_files[:10]), "\n    ... and",
                    len(input_files) - 10, "more files"]
        file_msg = map(str, file_msg)
        file_msg = " ".join(file_msg)
        logger.info(file_msg)
    else:
        for f in input_files:
            logger.info('    ' + f)

    ntuple_map = config.get('input', 'ntuple_map_file')
    with open(ntuple_map) as f:
        ntuple_map = yaml.load(f)
    load_L1TNTupleLibrary()
    reader = EventReader(
        input_files,
        ntuple_map,
        nevents=nevents,
        vectorized=config.try_get('analysis', 'vectorized', default=False),
        batch_size=config.try_get('analysis', 'batch_size', default=1),
    )

    logger.info(section.format("Preparing analysis modules"))
    results = [analyzer.prepare_for_events(reader) for analyzer in analyzers]
    check(results, analyzers, 'prepare_for_events')

    logger.info(section.format("Processing events"))
    # Fill the histograms from the tuples
    available_events = reader.numentries
    counter_rate = available_events // 10
    if nevents <= 10000 and not nevents < 0:
        counter_rate = nevents / 10
    for event in reader:
        processed_events = reader.processed_events
        if processed_events % counter_rate == 0:
            if nevents > 0:
                logger.info("{} of {}".format(processed_events, nevents))
            else:
                logger.info("{} of {}".format(processed_events, available_events))
        results = [p.produce(event) for p in producers]
        check(results, producers, 'produce')
        if filters:
            masks = [f(event) for f in filters]
            mask = create_event_mask(masks)
            event = event.mask(mask)
        if not event:
            logger.warning('Event did not pass any of the filters')
            continue
        results = [analyzer.process_event(processed_events, event) for analyzer in analyzers]
        check(results, analyzers, 'process_event')
        if all(results) is not True:
            break


@timerfunc_log_to(logger.info)
def process_histogram_files(config, analyzers):
    # Open the histogram files
    logger.info(section.format("Reading back histograms"))

    inputs = config.get('input', 'hist_files')
    logger.info("Inputs:")
    if len(inputs) > 10:
        msg = inputs[:10], "... and " + str(len(inputs) - 10) + " more"
        logger.info(msg)
    else:
        logger.info(inputs)

    # Open the histogram file
    results = {}
    for filename in inputs:
        for analyzer in analyzers:
            if analyzer not in results:
                results[analyzer] = True
            if analyzer.might_contain_histograms(filename):
                results[analyzer] &= bool(analyzer.reload_histograms(filename))

    # Check for errors
    return check(results.values(), results.keys(), 'process_histogram_files')


@timerfunc_log_to(logger.info)
def process_legacy(config, nevents, analyzers):
    logger.info(section.format("Running in legacy mode"))
    results = [analyzer.prepare_for_events(None) for analyzer in analyzers]
    check(results, analyzers, 'prepare_for_events')

    # setting entry = nevents  is ugly
    results = [analyzer.process_event(nevents, None) for analyzer in analyzers]
    check(results, analyzers, 'process_event')

    return all(results)


def run(config, nevents, reload_histograms):
    results = [False]
    # Fetch the analyzer
    analyzers = config.get('analysis', 'analyzers')
    out_cfg = config.get('output')
    analyzers = [load_analyzer(analyzer, out_cfg) for analyzer in analyzers]

    producers = config.get('analysis', 'producers')
    producers = [load_producer(producer, out_cfg) for producer in producers]
    _check_producer_outputs(producers)

    filters = config.get('analysis', 'filters')
    filters = [load_filter(f) for f in filters]

    if not reload_histograms:
        analysis_mode = config.try_get('analysis', 'mode', default='new')
        if analysis_mode == 'legacy':
            process_legacy(config, nevents, analyzers)
        else:
            process_tuples(config, nevents, analyzers, producers, filters)
    else:
        process_histogram_files(config, analyzers)

    # Write out the histograms
    for analyzer in analyzers:
        analyzer.write_histograms()

    # Turn the histograms to plots
    logging.getLogger("ROOT.TCanvas.Print").setLevel(logging.WARNING)
    logger.info(section.format("Making plots"))
    results = [analyzer.make_plots() for analyzer in analyzers]
    check(results, analyzers, 'make_plots')

    # Finalize
    logger.info(section.format("Finalizing things"))
    results = [analyzer.finalize() for analyzer in analyzers]
    check(results, analyzers, 'finalize')

    return all(results)


def _check_producer_outputs(producers):
    outputs = []
    for p in producers:
        for o in p._outputs:
            if o in outputs:
                msg = 'Producer output {} already defined by other producers'.format(
                    o)
                logger.error(msg)
                raise AttributeError(msg)
        outputs += p._outputs


def check(results, analyzers, method):
    msg = 'Problem during {method}() with analyzer "{analyzer}"'
    is_ok = all(results)
    if not is_ok:
        for i, r in enumerate(results):
            if r is not True:
                logger.error(msg.format(method=method, analyzer=analyzers[i]))
    return is_ok


@click.command()
@click.option('-c', '--config_file', help='YAML style config file', type=click.File(), required=True)
@click.option('-n', '--nevents', default=-1, help='Number of events to process.')
@click.option('-r', '--reload-histograms', is_flag=True,
              help="Reload histograms from a file and skip the input tuples")
@click.option('--hist-files', default=None,
              help="Provide a list of files to reload histograms from")
@click_log.simple_verbosity_option(logger)
def analyze(config_file, nevents, reload_histograms, hist_files):
    logger.info(section.format("Starting CMS L1T Analysis"))
    config = ConfigParser()
    config.read(config_file, reload_histograms, hist_files)

    isok = run(config, nevents, reload_histograms)

    logger.info('\n' + separator + '\n')
    if isok is not True:
        logger.error("There were errors during running:")
        logger.error(isok)
        logger.error('\n' + separator + '\n')


def load_analyzer(analyzer, output_cfg):
    name = analyzer['name']
    module = analyzer.pop('module')
    logger.debug("Try loading analyzer: {0} ({1})".format(name, module))
    module = import_module(module)
    logger.debug("Successfully loaded analyzer: {0} ({1})".format(name, module))
    cfg = dict(output_folder=output_cfg['folder'],
               plots_folder=output_cfg['plots_folder'],
               file_format=output_cfg.get('plot_format', 'pdf'),
               )
    cfg.update(analyzer)
    return module.Analyzer(**cfg)


def load_producer(producer, output_cfg):
    name = producer.pop('name')
    module = producer.pop('module')
    logger.debug("Try loading producer: {0} ({1})".format(name, module))
    module = import_module(module)
    logger.debug("Successfully loaded producer: {0} ({1})".format(name, module))
    # TODO: is this part needed for producers?
    cfg = dict(output_folder=output_cfg['folder'],
               plots_folder=output_cfg['plots_folder'],
               file_format=output_cfg.get('plot_format', 'pdf'),
               )
    cfg.update(producer)
    return module.Producer(**cfg)


def load_filter(filter):
    name = filter.pop('name')
    module = filter.pop('module')
    logger.debug("Try loading filter: {0} ({1})".format(name, module))
    tokens = module.split('.')
    module_path = '.'.join(tokens[:-1])
    module = import_module(module_path)
    caller = getattr(module, tokens[-1])
    logger.debug("Successfully loaded filter: {0} ({1})".format(name, caller))
    cfg = {}
    cfg.update(filter)
    return caller(**cfg)


if __name__ == '__main__':
    analyze()
